{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating RAG Systems with FloTorch\n",
    "\n",
    "[FloTorch](https://www.flotorch.ai/) offers a robust evaluation framework for Retrieval-Augmented Generation (RAG) systems, enabling comprehensive assessment and comparison of Large Language Models (LLMs). It focuses on key metrics such as accuracy, cost, and latency, crucial for enterprise-level deployments.\n",
    "\n",
    "## Key Evaluation Metrics for this Notebook\n",
    "\n",
    "In this notebook, we will focus on evaluating our RAG pipelines using the following metrics:\n",
    "\n",
    "* **Context Precision:** This metric quantifies the relevance of the retrieved context chunks. It's calculated as the average of the precision@k scores for each chunk within the retrieved context. Precision@k represents the proportion of relevant chunks within the top k retrieved chunks.\n",
    "\n",
    "* **Response Relevancy:** This metric assesses how well the generated response addresses the user's query. Higher scores indicate greater relevance and completeness, while lower scores suggest incompleteness or the inclusion of unnecessary information.\n",
    "\n",
    "* **Inference Cost:** This refers to the total cost incurred for invoking Bedrock models to generate responses for all entries in the ground truth dataset.\n",
    "\n",
    "* **Latency:** This measures the time taken for the inference process, specifically the duration of the Bedrock model invocations.\n",
    "\n",
    "## Leveraging Ragas for Evaluation\n",
    "\n",
    "This evaluation process utilizes [Ragas](https://docs.ragas.io/en/stable/), a powerful library designed to streamline and enhance the evaluation of Large Language Model (LLM) applications, allowing for confident and straightforward assessment.\n",
    "\n",
    "Ragas utilizes Large Language Models (LLMs) internally to compute both Context Precision and Response Relevancy scores. In this evaluation, we will specifically employ `amazon.titan-embed-text-v1` for generating embeddings and `cohere.command-r-plus-v1:0` for the inference tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../Lab 1/variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_config_data = {\n",
    "   \"eval_embedding_model\" : \"amazon.titan-embed-text-v2:0\",\n",
    "   \"eval_retrieval_model\" : \"us.amazon.nova-micro-v1:0\",\n",
    "   \"eval_retrieval_service\" : \"bedrock\",\n",
    "   \"aws_region\" : variables['regionName'],\n",
    "   \"eval_embed_vector_dimension\" : 1024,\n",
    "   \"retrieval_model\": \"us.amazon.nova-lite-v1:0\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load RAG response data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Dict\n",
    "from flotorch_core.evaluator.evaluation_item import EvaluationItem\n",
    "\n",
    "def convert_to_evaluation_dict(data: Dict) -> Dict[str, List[EvaluationItem]]:\n",
    "    \"\"\"\n",
    "    Converts the given dictionary into a dictionary where the key is the KB type\n",
    "    and the value is a list of EvaluationItem objects.  This version handles\n",
    "    dynamic KB keys.\n",
    "\n",
    "    Args:\n",
    "        data: The input dictionary.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where the key is the KB type and the value is a list of EvaluationItem.\n",
    "    \"\"\"\n",
    "\n",
    "    evaluation_dict: Dict[str, List[EvaluationItem]] = {}\n",
    "\n",
    "    for kb_type, items in data.items():\n",
    "        if isinstance(items, list):  # Ensure we're processing a list of items\n",
    "            evaluation_items: List[EvaluationItem] = []\n",
    "            for item_data in items:\n",
    "                if isinstance(item_data, dict) and \"question\" in item_data and \"expected_answer\" in item_data and \"generated_answer\" in item_data and \"retrieved_contexts\" in item_data:\n",
    "                    evaluation_item = EvaluationItem(\n",
    "                        question=item_data[\"question\"],\n",
    "                        generated_answer=item_data[\"generated_answer\"],\n",
    "                        expected_answer=item_data[\"expected_answer\"],\n",
    "                        context=[context[\"text\"] for context in item_data[\"retrieved_contexts\"]]\n",
    "                    )\n",
    "                    evaluation_items.append(evaluation_item)\n",
    "            evaluation_dict[kb_type] = evaluation_items\n",
    "\n",
    "    return evaluation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filename = f\"../results/rag_evaluation_responses.json\"\n",
    "\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    loaded_responses = json.load(f)\n",
    "\n",
    "evaluation_dataset_per_kb = convert_to_evaluation_dict(loaded_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_evaluation = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Evaluation with Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flotorch_core.embedding.embedding_registry import embedding_registry\n",
    "from flotorch_core.embedding.titanv2_embedding import TitanV2Embedding\n",
    "from flotorch_core.embedding.cohere_embedding import CohereEmbedding\n",
    "from flotorch_core.inferencer.inferencer_provider_factory import InferencerProviderFactory\n",
    "from flotorch_core.evaluator.ragas_evaluator import RagasEvaluator\n",
    "import numpy as np\n",
    "\n",
    "# Initialize embeddings\n",
    "embedding_class = embedding_registry.get_model(evaluation_config_data.get(\"eval_embedding_model\"))\n",
    "embedding = embedding_class(evaluation_config_data.get(\"eval_embedding_model\"), \n",
    "                            evaluation_config_data.get(\"aws_region\"), \n",
    "                            int(evaluation_config_data.get(\"eval_embed_vector_dimension\"))\n",
    "                            )\n",
    "\n",
    "# Initialize inferencer\n",
    "inferencer = InferencerProviderFactory.create_inferencer_provider(\n",
    "    False,\"\",\"\",\n",
    "    evaluation_config_data.get(\"eval_retrieval_service\"),\n",
    "    evaluation_config_data.get(\"eval_retrieval_model\"), \n",
    "    evaluation_config_data.get(\"aws_region\"), \n",
    "    variables['bedrockExecutionRoleArn'],\n",
    "    float(0.1)\n",
    ")\n",
    "\n",
    "evaluator = RagasEvaluator(inferencer, embedding)\n",
    "\n",
    "for evaluation_dataset_kb_id in evaluation_dataset_per_kb:\n",
    "    ragas_report = evaluator.evaluate(evaluation_dataset_per_kb[evaluation_dataset_kb_id])\n",
    "    final_evaluation[evaluation_dataset_kb_id] = {\n",
    "        'llm_context_precision_with_reference': np.mean(ragas_report['llm_context_precision_with_reference']),\n",
    "        'faithfulness': np.mean(ragas_report['faithfulness']),\n",
    "        'answer_relevancy': np.mean(ragas_report['answer_relevancy'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost and Latency Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "MILLION = 1_000_000\n",
    "THOUSAND = 1_000\n",
    "SECONDS_IN_MINUTE = 60\n",
    "MINUTES_IN_HOUR = 60\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('../data/bedrock_limits_small.csv')\n",
    "\n",
    "def calculate_bedrock_inference_cost(input_tokens,output_tokens, inference_model, aws_region):\n",
    "\n",
    "    input_price = df[\n",
    "        (df[\"model\"] == inference_model) & (df[\"Region\"] == aws_region)\n",
    "        ][\"input_price\"]\n",
    "\n",
    "    output_price = df[\n",
    "        (df[\"model\"] == inference_model) & (df[\"Region\"] == aws_region)\n",
    "        ][\"output_price\"]\n",
    "\n",
    "    input_price_per_million_tokens = float(input_price.values[0])  # Price per million tokens\n",
    "    output_price_per_million_tokens = float(output_price.values[0])  # Price per million tokens\n",
    "\n",
    "    input_actual_cost = (input_price_per_million_tokens * float(input_tokens)) / MILLION\n",
    "    output_actual_cost = (output_price_per_million_tokens * float(output_tokens)) / MILLION\n",
    "    return input_actual_cost + output_actual_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List\n",
    "from decimal import Decimal\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class MetricsData:\n",
    "    \"\"\"Data class to store metrics information\"\"\"\n",
    "    cost: Decimal\n",
    "    latency: float\n",
    "    input_tokens: int\n",
    "    output_tokens: int\n",
    "\n",
    "def extract_metadata_metrics(metadata: Dict[str, Any]) -> MetricsData:\n",
    "    \"\"\"\n",
    "    Extract metrics from metadata dictionary\n",
    "    \n",
    "    Args:\n",
    "        metadata: Dictionary containing metadata information\n",
    "    Returns:\n",
    "        MetricsData object with extracted metrics\n",
    "    \"\"\"\n",
    "    return MetricsData(\n",
    "        input_tokens=metadata.get(\"inputTokens\", 0),\n",
    "        output_tokens=metadata.get(\"outputTokens\", 0),\n",
    "        latency=float(metadata.get(\"latencyMs\", 0)),\n",
    "        cost=Decimal('0.0000')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kb_type, items in loaded_responses.items():\n",
    "    if isinstance(items, list):\n",
    "        continue\n",
    "\n",
    "    total_cost = Decimal('0.0000')\n",
    "    total_latency = 0.0\n",
    "    item_count = 0\n",
    "\n",
    "    for item_data in items:\n",
    "        if not isinstance(item_data, dict) or \"metadata\" not in item_data:\n",
    "            continue\n",
    "\n",
    "        metrics = extract_metadata_metrics(item_data[\"metadata\"])\n",
    "        \n",
    "        # Calculate cost for this item\n",
    "        item_cost = calculate_bedrock_inference_cost(\n",
    "            metrics.input_tokens,\n",
    "            metrics.output_tokens,\n",
    "            evaluation_config_data[\"retrieval_model\"],\n",
    "            evaluation_config_data[\"aws_region\"]\n",
    "        )\n",
    "        \n",
    "        total_cost += Decimal(str(item_cost))\n",
    "        total_latency += metrics.latency\n",
    "        item_count += 1\n",
    "\n",
    "    if item_count > 0:\n",
    "         # Calculate averages\n",
    "        if kb_type not in final_evaluation:\n",
    "            # Insert - key doesn't exist yet\n",
    "            final_evaluation[kb_type] = {\n",
    "                'cost': float(total_cost),\n",
    "                'average_cost': float(total_cost / item_count),\n",
    "                'latency': total_latency,\n",
    "                'average_latency': total_latency / item_count,\n",
    "                'processed_items': item_count\n",
    "            }\n",
    "        else:\n",
    "            # Update - key already exists\n",
    "            final_evaluation[kb_type].update({\n",
    "                'cost': float(total_cost),\n",
    "                'average_cost': float(total_cost / item_count),\n",
    "                'latency': total_latency,\n",
    "                'average_latency': total_latency / item_count,\n",
    "                'processed_items': item_count\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics as pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the nested dictionary to a DataFrame\n",
    "df = pd.DataFrame.from_dict(final_evaluation, orient='index')\n",
    "\n",
    "# If you want the kb_type as a column instead of an index\n",
    "df = df.reset_index().rename(columns={'index': 'kb_type'})\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
