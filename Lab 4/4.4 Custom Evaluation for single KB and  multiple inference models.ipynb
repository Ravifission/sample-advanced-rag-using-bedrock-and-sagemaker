{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating RAG Systems with FloTorch\n",
    "\n",
    "[FloTorch](https://www.flotorch.ai/) offers a robust evaluation framework for Retrieval-Augmented Generation (RAG) systems, enabling comprehensive assessment and comparison of Large Language Models (LLMs). It focuses on key metrics such as accuracy, cost, and latency, crucial for enterprise-level deployments.\n",
    "\n",
    "## Key Evaluation Metrics for this Notebook\n",
    "\n",
    "In this notebook, we will focus on evaluating our RAG pipelines using the following metrics:\n",
    "\n",
    "* **Correctness:** This refers to the total number of samples that semantically both generated and expected are mateched\n",
    "\n",
    "* **Inference Cost:** This refers to the total cost incurred for invoking Bedrock models to generate responses for all entries in the ground truth dataset.\n",
    "\n",
    "* **Latency:** This measures the time taken for the inference process, specifically the duration of the Bedrock model invocations.\n",
    "\n",
    "\n",
    "RAG systems are evaluated using a scoring method that measures response quality to questions in the evaluation set. Responses are rated as correct, Missing or incorrect:\n",
    "\n",
    "- correct: The response correctly answers the user question and contains no hallucinated content.\n",
    "\n",
    "- Missing: The answer does not provide the requested information. Such as “I don’t know”, “I’m sorry I can’t find …” or similar sentences without providing a concrete answer to the question.\n",
    "\n",
    "- Incorrect: The response provides wrong or irrelevant information to answer the user question\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accountNumber': '746074413210',\n",
       " 'regionName': 'us-west-2',\n",
       " 'collectionArn': 'arn:aws:aoss:us-west-2:746074413210:collection/3f35uv3lze9bdothrm0c',\n",
       " 'collectionId': '3f35uv3lze9bdothrm0c',\n",
       " 'vectorIndexName': 'ws-index-',\n",
       " 'bedrockExecutionRoleArn': 'arn:aws:iam::746074413210:role/advanced-rag-workshop-bedrock_execution_role-us-west-2',\n",
       " 's3Bucket': '746074413210-us-west-2-advanced-rag-workshop',\n",
       " 'kbFixedChunk': 'WO4U6AWAU1',\n",
       " 'kbSemanticChunk': 'OUFEWBGEES',\n",
       " 'kbHierarchicalChunk': 'IHWIS6EP0H'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(\"../Lab 1/variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_config_data = {\n",
    "   \"eval_embedding_model\" : \"amazon.titan-embed-text-v2:0\",\n",
    "   \"eval_retrieval_model\" : \"bedrock/cohere.command-r-v1:0\",\n",
    "   \"eval_retrieval_service\" : \"bedrock\",\n",
    "   \"aws_region\" : variables['regionName'],\n",
    "   \"eval_embed_vector_dimension\" : 1024\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load RAG response data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filename = f\"../results/ragas_evaluation_responses_for_different_models.json\"\n",
    "\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    loaded_responses = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Evaluation with Custom Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_evaluator import CustomEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:28<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation completed for us.amazon.nova-micro-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:28<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation completed for us.amazon.nova-pro-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:35<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation completed for us.anthropic.claude-3-5-haiku-20241022-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:29<00:00,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation completed for us.anthropic.claude-3-5-sonnet-20241022-v2:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "processor = CustomEvaluator(evaluator_llm_info = evaluation_config_data)\n",
    "evaluation_metrics = {}\n",
    "for model_id, inference_data in loaded_responses.items():\n",
    "    results = processor.evaluate(inference_data)\n",
    "    evaluation_metrics[model_id] = results\n",
    "    print(f\"Evaluation completed for {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_evaluation = evaluator.evaluate_results(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost and Latency Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inference_cost': 0.0038132850000000005, 'average_inference_cost': 0.00019066425, 'latency': 19610.0, 'average_latency': 980.5, 'processed_items': 20}\n",
      "{'inference_cost': 0.0855448, 'average_inference_cost': 0.00427724, 'latency': 23764.0, 'average_latency': 1188.2, 'processed_items': 20}\n",
      "{'inference_cost': 0.02659136, 'average_inference_cost': 0.001329568, 'latency': 132016.0, 'average_latency': 6600.8, 'processed_items': 20}\n",
      "{'inference_cost': 0.381432, 'average_inference_cost': 0.0190716, 'latency': 127302.0, 'average_latency': 6365.1, 'processed_items': 20}\n"
     ]
    }
   ],
   "source": [
    "from cost_compute_utils import calculate_cost_and_latency_metrics\n",
    "\n",
    "for model in loaded_responses:\n",
    "    inference_data = loaded_responses[model]\n",
    "    cost_and_latency_metrics = calculate_cost_and_latency_metrics(inference_data, model,\n",
    "                evaluation_config_data[\"aws_region\"])\n",
    "    print(cost_and_latency_metrics)\n",
    "    if model not in final_evaluation:\n",
    "        # Insert - key doesn't exist yet\n",
    "        final_evaluation[model] = cost_and_latency_metrics\n",
    "    else:\n",
    "        # Update - key already exists\n",
    "        final_evaluation[model].update(cost_and_latency_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics as pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>number of samples correct</th>\n",
       "      <th>inference_cost</th>\n",
       "      <th>average_inference_cost</th>\n",
       "      <th>latency</th>\n",
       "      <th>average_latency</th>\n",
       "      <th>processed_items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>us.amazon.nova-micro-v1:0</td>\n",
       "      <td>17</td>\n",
       "      <td>0.003813</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>19610.0</td>\n",
       "      <td>980.5</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>us.amazon.nova-pro-v1:0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.085545</td>\n",
       "      <td>0.004277</td>\n",
       "      <td>23764.0</td>\n",
       "      <td>1188.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>us.anthropic.claude-3-5-haiku-20241022-v1:0</td>\n",
       "      <td>17</td>\n",
       "      <td>0.026591</td>\n",
       "      <td>0.001330</td>\n",
       "      <td>132016.0</td>\n",
       "      <td>6600.8</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>us.anthropic.claude-3-5-sonnet-20241022-v2:0</td>\n",
       "      <td>15</td>\n",
       "      <td>0.381432</td>\n",
       "      <td>0.019072</td>\n",
       "      <td>127302.0</td>\n",
       "      <td>6365.1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          model  number of samples correct  \\\n",
       "0                     us.amazon.nova-micro-v1:0                         17   \n",
       "1                       us.amazon.nova-pro-v1:0                         20   \n",
       "2   us.anthropic.claude-3-5-haiku-20241022-v1:0                         17   \n",
       "3  us.anthropic.claude-3-5-sonnet-20241022-v2:0                         15   \n",
       "\n",
       "   inference_cost  average_inference_cost   latency  average_latency  \\\n",
       "0        0.003813                0.000191   19610.0            980.5   \n",
       "1        0.085545                0.004277   23764.0           1188.2   \n",
       "2        0.026591                0.001330  132016.0           6600.8   \n",
       "3        0.381432                0.019072  127302.0           6365.1   \n",
       "\n",
       "   processed_items  \n",
       "0               20  \n",
       "1               20  \n",
       "2               20  \n",
       "3               20  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the nested dictionary to a DataFrame\n",
    "evaluation_df = pd.DataFrame.from_dict(final_evaluation, orient='index')\n",
    "\n",
    "# If you want the kb_type as a column instead of an index\n",
    "evaluation_df = evaluation_df.reset_index().rename(columns={'index': 'model'})\n",
    "\n",
    "evaluation_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
