{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval and Generation using Bedrock Models for Ground Truth\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook provides a practical demonstration of Retrieval-Augmented Generation (RAG) with Amazon Bedrock foundational models, utilizing FloTorch for ground truth evaluation. It walks through the process of fetching pertinent information from a knowledge base and subsequently generating responses grounded in the retrieved context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../Lab 1/variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load prompt.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_file_path = '../data/prompt.json'\n",
    "with open(prompt_file_path, 'r') as f:\n",
    "    prompt = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch all available bedrock kb id's\n",
    "\n",
    "**Important:** This step assumes that your knowledge base has already been created in Lab 1. Please ensure that you have completed the knowledge base creation as part of Lab 1 before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_kbs = [] \n",
    "\n",
    "kb_types = ['kbFixedChunk', 'kbSemanticChunk', 'kbHierarchicalChunk', 'kbCustomChunk']\n",
    "bedrock_kbs = [variables[kb_type] for kb_type in kb_types if variables.get(kb_type)]\n",
    "\n",
    "bedrock_kbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Config\n",
    "\n",
    "For consistent evaluation across different knowledge bases, we will use the following fixed parameters for retrieval and inference:\n",
    "\n",
    "* **KNN (k-Nearest Neighbors):** 5\n",
    "* **Rerank Model:** Amazon Rerank\n",
    "* **N-Shot Prompt:** 1\n",
    "* **Inference Model:** Nova-Lite (via Bedrock)\n",
    "* **Temperature:** 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_config_data = {\n",
    "    \"temp_retrieval_llm\": \"0.1\",\n",
    "    \"gt_data\": variables[\"s3_ground_truth_path\"],\n",
    "    \"rerank_model_id\": \"amazon.rerank-v1:0\",\n",
    "    \"retrieval_service\": \"bedrock\",\n",
    "    \"knn_num\": \"5\",\n",
    "    \"retrieval_model\": \"us.amazon.nova-lite-v1:0\",\n",
    "    \"aws_region\": \"us-east-1\",\n",
    "    \"n_shot_prompt_guide_obj\": prompt,\n",
    "    \"n_shot_prompts\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ground truth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flotorch_core.storage.storage_provider_factory import StorageProviderFactory\n",
    "from flotorch_core.reader.json_reader import JSONReader\n",
    "from flotorch_core.chunking.chunking import Chunk\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Question(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "    def get_chunk(self) -> Chunk:\n",
    "        return Chunk(data=self.question)\n",
    "\n",
    "\n",
    "gt_data = exp_config_data['gt_data']\n",
    "storage = StorageProviderFactory.create_storage_provider(gt_data)\n",
    "gt_data_path = storage.get_path(gt_data)\n",
    "json_reader = JSONReader(storage)\n",
    "questions = json_reader.read_as_model(gt_data_path, Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flotorch_core.rerank.rerank import BedrockReranker\n",
    "\n",
    "reranker = BedrockReranker(exp_config_data.get(\"aws_region\"), exp_config_data.get(\"rerank_model_id\")) \\\n",
    "    if exp_config_data.get(\"rerank_model_id\").lower() != \"none\" \\\n",
    "    else None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Bedrock Inferencer\n",
    "\n",
    "Creates and returns an appropriate `Inferencer` instance depending on service and the model \n",
    "\n",
    "### Parameters\n",
    "\n",
    "- `gateway_enabled`: *(bool)* – Enables FloTorch LLM gateway-based invocation if set to `True`.\n",
    "- `gateway_url`: *(str)* – URL endpoint for the FloTorch LLM Gateway.\n",
    "- `gateway_api_key`: *(str)* – API key for authenticating requests to the FloTorch LLM gateway.\n",
    "- `retrieval_service`: *(str)* – Name of the retrieval service (e.g., bedrock, sagemaker).\n",
    "- `retrieval_model`: *(str)* – The model to use for inference (e.g., `anthropic.claude-v2`).\n",
    "- `aws_region`: *(str)* – AWS region for service provisioning (e.g., `us-east-1`).\n",
    "- `iam_role`: *(str)* – IAM role ARN for SageMaker invocation permissions.\n",
    "- `n_shot_prompts`: *(int)* – Number of few-shot examples to include in prompt.\n",
    "- `temp_retrieval_llm`: *(float)* – Temperature setting for the language model.\n",
    "- `n_shot_prompt_guide_obj`: *(Any)* – Few-shot guide object for prompt engineering.\n",
    "\n",
    "---\n",
    "\n",
    "### Behavior\n",
    "\n",
    "- If `gateway_enabled` is `True`, connects to the specified API Gateway using credentials.\n",
    "- If disabled, falls back to direct model invocation through supported services like AWS Bedrock.\n",
    "- Supports dynamic few-shot prompting and custom temperature configuration.\n",
    "\n",
    "---\n",
    "\n",
    "### Outcome\n",
    "\n",
    "Returns a fully configured `Inferencer` object capable of generating answers or completions for queries using the selected language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flotorch_core.inferencer.inferencer_provider_factory import InferencerProviderFactory\n",
    "\n",
    "inferencer = InferencerProviderFactory.create_inferencer_provider(\n",
    "    False,\"\",\"\",\n",
    "    exp_config_data.get(\"retrieval_service\"),\n",
    "    exp_config_data.get(\"retrieval_model\"), \n",
    "    exp_config_data.get(\"aws_region\"), \n",
    "    variables['bedrockExecutionRoleArn'],\n",
    "    int(exp_config_data.get(\"n_shot_prompts\", 0)), \n",
    "    float(exp_config_data.get(\"temp_retrieval_llm\", 0)), \n",
    "    exp_config_data.get(\"n_shot_prompt_guide_obj\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG with ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_flotorch(vector_storage, questions: list[Question]):\n",
    "    \"\"\"\n",
    "        Process a list of questions through the RAG pipeline.\n",
    "\n",
    "        Args:\n",
    "            vector_storage: Initialized vector storage \n",
    "            questions: List of Question objects to process\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, Any]]: List of responses containing metadata and answers\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If vector storage search fails\n",
    "            RuntimeError: If text generation fails\n",
    "    \"\"\"\n",
    "    responses_list = []\n",
    "    for question in questions:\n",
    "        try:\n",
    "            inference_response = {}\n",
    "            # fetch documents from bedrock knowledge bases\n",
    "            question_chunk = question.get_chunk()\n",
    "            response = vector_storage.search(question_chunk, int(exp_config_data.get(\"knn_num\")))\n",
    "            vector_response = response.to_json()['result']\n",
    "            \n",
    "            # rerank if selected\n",
    "            if reranker:\n",
    "                vector_response = reranker.rerank_documents(question_chunk.data, vector_response)\n",
    "            \n",
    "            # send for inferencing\n",
    "            metadata, answer = inferencer.generate_text(question.question, vector_response)\n",
    "\n",
    "            inference_response[\"metadata\"] = metadata\n",
    "            inference_response[\"answer\"] = answer\n",
    "            inference_response[\"question\"] = question.question\n",
    "            inference_response[\"retrieved_contexts\"] = vector_response\n",
    "            responses_list.append(inference_response)\n",
    "        except Exception as e:\n",
    "            # Log the error and continue with next question\n",
    "            print(f\"Error processing question: {question.question}. Error: {str(e)}\")\n",
    "            responses_list.append({\n",
    "                \"metadata\": {\"error\": str(e)},\n",
    "                \"answer\": \"Failed to process question\"\n",
    "            })\n",
    "\n",
    "    return responses_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flotorch_core.storage.db.vector.vector_storage_factory import VectorStorageFactory\n",
    "\n",
    "rag_response_dict = {}\n",
    "for bedrock_kb in bedrock_kbs:\n",
    "    vector_storage = VectorStorageFactory.create_vector_storage(\n",
    "        knowledge_base=True,\n",
    "        use_bedrock_kb=True,\n",
    "        embedding=None,\n",
    "        knowledge_base_id=bedrock_kb,\n",
    "        aws_region=exp_config_data.get(\"aws_region\")\n",
    "    )\n",
    "\n",
    "    responses = rag_with_flotorch(vector_storage, questions)\n",
    "    rag_response_dict[bedrock_kb] = responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the results to a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filename = f\"../results/rag_evaluation_responses.json\"\n",
    "\n",
    "# Save to JSON with proper formatting\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(rag_response_dict, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
