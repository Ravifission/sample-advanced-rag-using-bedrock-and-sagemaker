{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval and Generation using Bedrock Models for Ground Truth\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook provides a practical demonstration of Retrieval-Augmented Generation (RAG) with Amazon Bedrock foundational models, utilizing FloTorch for ground truth evaluation. It walks through the process of fetching pertinent information from a knowledge base and subsequently generating responses grounded in the retrieved context.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "1.  Confirm that all prerequisites outlined in the `1.1 Prerequisites.ipynb` notebook from Lab 1 have been completed.\n",
    "2.  Ensure that at least one of the knowledge base creation notebooks (`1.2`, `1.3`, `1.4`, or `1.5`) from Lab 1 has been successfully executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../Lab 1/variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load prompt.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_file_path = '../data/prompt.json'\n",
    "with open(prompt_file_path, 'r') as f:\n",
    "    prompt = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the evaluation against Fixed Chunking KB\n",
    "\n",
    "**Important:** This step assumes that your knowledge base has already been created in Lab 1. Please ensure that you have completed the knowledge base creation as part of Lab 1 before proceeding.\n",
    "\n",
    "Inference Models considered - Amazon Nova Micro, Amazon Nova Pro, Claude Haiku 3.5, Claude Sonnet 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_kb = variables['kbFixedChunk']\n",
    "\n",
    "inference_models = [\"us.amazon.nova-lite-v1:0\",\"us.amazon.nova-pro-v1:0\",\n",
    "                \"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\"us.anthropic.claude-3-5-sonnet-20241022-v2:0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Config\n",
    "\n",
    "* **Bedrock KB Id:** KnowledgeBase Id to query against\n",
    "* **KNN (k-Nearest Neighbors):** 5\n",
    "* **Rerank Model:** Amazon Rerank\n",
    "* **N-Shot Prompt:** 1\n",
    "* **Temperature:** 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_config_data = {\n",
    "    \"bedrock_kb_id\": bedrock_kb,\n",
    "    \"temp_retrieval_llm\": \"0.1\",\n",
    "    \"gt_data\": variables[\"s3_ground_truth_path\"],\n",
    "    \"rerank_model_id\": \"amazon.rerank-v1:0\",\n",
    "    \"retrieval_service\": \"bedrock\",\n",
    "    \"knn_num\": \"5\",\n",
    "    \"retrieval_model\": \"us.amazon.nova-lite-v1:0\",\n",
    "    \"aws_region\": variables['regionName'],\n",
    "    \"n_shot_prompt_guide_obj\": prompt,\n",
    "    \"n_shot_prompts\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ground truth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flotorch_core.storage.storage_provider_factory import StorageProviderFactory\n",
    "from flotorch_core.reader.json_reader import JSONReader\n",
    "from flotorch_rag_utils import Question\n",
    "\n",
    "gt_data = exp_config_data['gt_data']\n",
    "storage = StorageProviderFactory.create_storage_provider(gt_data)\n",
    "gt_data_path = storage.get_path(gt_data)\n",
    "json_reader = JSONReader(storage)\n",
    "questions = json_reader.read_as_model(gt_data_path, Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize VectorStorage (in this case Bedrock KnowledgeBases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flotorch_core.storage.db.vector.vector_storage_factory import VectorStorageFactory\n",
    "\n",
    "vector_storage = VectorStorageFactory.create_vector_storage(\n",
    "        knowledge_base=True,\n",
    "        use_bedrock_kb=True,\n",
    "        embedding=None,\n",
    "        knowledge_base_id=exp_config_data.get(\"bedrock_kb_id\"),\n",
    "        aws_region=exp_config_data.get(\"aws_region\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flotorch_core.rerank.rerank import BedrockReranker\n",
    "\n",
    "reranker = BedrockReranker(exp_config_data.get(\"aws_region\"), exp_config_data.get(\"rerank_model_id\")) \\\n",
    "    if exp_config_data.get(\"rerank_model_id\").lower() != \"none\" \\\n",
    "    else None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute RAG against all the inference models\n",
    "\n",
    "Initialize inferencer and then perform the retrieval, reranking, and inference steps using the `flotorch-core` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flotorch_core.inferencer.inferencer_provider_factory import InferencerProviderFactory\n",
    "from flotorch_rag_utils import rag_with_flotorch\n",
    "\n",
    "rag_response_dict = {}\n",
    "\n",
    "# The evaluation process duration is dependent on the volume of questions and the number of models bases being evaluated. \n",
    "# Larger evaluations require more time, generally around 5-6 minutes.\n",
    "for inference_model in inference_models:\n",
    "    inferencer = InferencerProviderFactory.create_inferencer_provider(\n",
    "        False,\"\",\"\",\n",
    "        exp_config_data.get(\"retrieval_service\"),\n",
    "        inference_model, \n",
    "        exp_config_data.get(\"aws_region\"), \n",
    "        variables['bedrockExecutionRoleArn'],\n",
    "        int(exp_config_data.get(\"n_shot_prompts\", 0)), \n",
    "        float(exp_config_data.get(\"temp_retrieval_llm\", 0)), \n",
    "        exp_config_data.get(\"n_shot_prompt_guide_obj\")\n",
    "    )\n",
    "\n",
    "    responses = rag_with_flotorch(exp_config_data, vector_storage, reranker, inferencer, questions)\n",
    "    rag_response_dict[inference_model] = responses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the results to a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filename = f\"../results/ragas_evaluation_responses_for_different_models.json\"\n",
    "\n",
    "# Save to JSON with proper formatting\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(rag_response_dict, f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
