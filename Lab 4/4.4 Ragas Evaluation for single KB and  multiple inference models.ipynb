{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating RAG Systems with FloTorch\n",
    "\n",
    "[FloTorch](https://www.flotorch.ai/) offers a robust evaluation framework for Retrieval-Augmented Generation (RAG) systems, enabling comprehensive assessment and comparison of Large Language Models (LLMs). It focuses on key metrics such as accuracy, cost, and latency, crucial for enterprise-level deployments.\n",
    "\n",
    "## Key Evaluation Metrics for this Notebook\n",
    "\n",
    "In this notebook, we will focus on evaluating our RAG pipelines using the following metrics:\n",
    "\n",
    "* **Context Precision:** This metric quantifies the relevance of the retrieved context chunks. It's calculated as the average of the precision@k scores for each chunk within the retrieved context. Precision@k represents the proportion of relevant chunks within the top k retrieved chunks.\n",
    "\n",
    "* **Response Relevancy:** This metric assesses how well the generated response addresses the user's query. Higher scores indicate greater relevance and completeness, while lower scores suggest incompleteness or the inclusion of unnecessary information.\n",
    "\n",
    "* **Inference Cost:** This refers to the total cost incurred for invoking Bedrock models to generate responses for all entries in the ground truth dataset.\n",
    "\n",
    "* **Latency:** This measures the time taken for the inference process, specifically the duration of the Bedrock model invocations.\n",
    "\n",
    "## Leveraging Ragas for Evaluation\n",
    "\n",
    "This evaluation process utilizes [Ragas](https://docs.ragas.io/en/stable/), a powerful library designed to streamline and enhance the evaluation of Large Language Model (LLM) applications, allowing for confident and straightforward assessment.\n",
    "\n",
    "Ragas utilizes Large Language Models (LLMs) internally to compute both Context Precision and Response Relevancy scores. In this evaluation, we will specifically employ `amazon.titan-embed-text-v2` for generating embeddings and `amazon.nova-lite-v1:0` for the inference tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../Lab 1/variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_config_data = {\n",
    "   \"eval_embedding_model\" : \"amazon.titan-embed-text-v2:0\",\n",
    "   \"eval_retrieval_model\" : \"us.amazon.nova-lite-v1:0\",\n",
    "   \"eval_retrieval_service\" : \"bedrock\",\n",
    "   \"aws_region\" : variables['regionName'],\n",
    "   \"eval_embed_vector_dimension\" : 1024\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load RAG response data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from evaluation_utils import convert_to_evaluation_dict\n",
    "\n",
    "filename = f\"../results/ragas_evaluation_responses_for_different_models.json\"\n",
    "\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    loaded_responses = json.load(f)\n",
    "\n",
    "evaluation_dataset_per_model = convert_to_evaluation_dict(loaded_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_evaluation = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Evaluation with Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flotorch_core.embedding.embedding_registry import embedding_registry\n",
    "from flotorch_core.embedding.titanv2_embedding import TitanV2Embedding\n",
    "from flotorch_core.embedding.cohere_embedding import CohereEmbedding\n",
    "from flotorch_core.inferencer.inferencer_provider_factory import InferencerProviderFactory\n",
    "from flotorch_core.evaluator.ragas_evaluator import RagasEvaluator\n",
    "\n",
    "# Initialize embeddings\n",
    "embedding_class = embedding_registry.get_model(evaluation_config_data.get(\"eval_embedding_model\"))\n",
    "embedding = embedding_class(evaluation_config_data.get(\"eval_embedding_model\"), \n",
    "                            evaluation_config_data.get(\"aws_region\"), \n",
    "                            int(evaluation_config_data.get(\"eval_embed_vector_dimension\"))\n",
    "                            )\n",
    "\n",
    "# Initialize inferencer\n",
    "inferencer = InferencerProviderFactory.create_inferencer_provider(\n",
    "    False,\"\",\"\",\n",
    "    evaluation_config_data.get(\"eval_retrieval_service\"),\n",
    "    evaluation_config_data.get(\"eval_retrieval_model\"), \n",
    "    evaluation_config_data.get(\"aws_region\"), \n",
    "    variables['bedrockExecutionRoleArn'],\n",
    "    float(0.1)\n",
    ")\n",
    "\n",
    "evaluator = RagasEvaluator(inferencer, embedding)\n",
    "\n",
    "for model in evaluation_dataset_per_model:\n",
    "    ragas_report = evaluator.evaluate(evaluation_dataset_per_model[model])\n",
    "    if ragas_report:\n",
    "        eval_metrics = ragas_report._repr_dict\n",
    "        eval_metrics = {key: round(value, 2) if isinstance(value, float) else value for key, value in eval_metrics.items()} \n",
    "    final_evaluation[model] = {\n",
    "            'llm_context_precision_with_reference': eval_metrics['llm_context_precision_with_reference'],\n",
    "            'faithfulness': eval_metrics['faithfulness'],\n",
    "            'answer_relevancy': eval_metrics['answer_relevancy']\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost and Latency Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cost_compute_utils import calculate_cost_and_latency_metrics\n",
    "\n",
    "for model in loaded_responses.items():\n",
    "    inference_data = loaded_responses[model]\n",
    "    cost_and_latency_metrics = calculate_cost_and_latency_metrics(inference_data, model,\n",
    "                evaluation_config_data[\"aws_region\"])\n",
    "    \n",
    "    if model not in final_evaluation:\n",
    "        # Insert - key doesn't exist yet\n",
    "        final_evaluation[model] = cost_and_latency_metrics\n",
    "    else:\n",
    "        # Update - key already exists\n",
    "        final_evaluation[model].update(cost_and_latency_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics as pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the nested dictionary to a DataFrame\n",
    "evaluation_df = pd.DataFrame.from_dict(final_evaluation, orient='index')\n",
    "\n",
    "# If you want the kb_type as a column instead of an index\n",
    "evaluation_df = evaluation_df.reset_index().rename(columns={'index': 'model'})\n",
    "\n",
    "print(evaluation_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
